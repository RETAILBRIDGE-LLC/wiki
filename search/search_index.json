{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to VocaSync Studios","text":"<p>VocaSync Studios is an AI-driven platform revolutionizing the way content is dubbed, localized, and personalized across the globe. Whether you're a filmmaker, content creator, streaming platform, or advertising agency, our mission is to help your content speak every language, with the right emotion, and the perfect voice.</p>"},{"location":"#our-objective","title":"\ud83c\udfaf Our Objective","text":"<p>VocaSync Studios is on a mission to redefine audio-visual storytelling by:</p> <ul> <li>Delivering hyper-personalized dubbing using state-of-the-art AI models</li> <li>Supporting emotionally-aware voice synthesis for lifelike performances</li> <li>Enabling seamless real-time or batch dubbing workflows</li> <li>Offering developer-friendly APIs for integration into existing media pipelines</li> <li>Creating inclusive, multilingual experiences for global audiences</li> </ul>"},{"location":"#who-is-this-for","title":"\ud83d\udc65 Who Is This For?","text":"<p>VocaSync is designed for:</p> <ul> <li>\ud83c\udfac Film &amp; TV Studios \u2013 Streamline localization for international releases</li> <li>\ud83d\udcf1 OTT Platforms &amp; YouTubers \u2013 Scale dubbing across multiple regions and languages</li> <li>\ud83c\udfa4 Voiceover Artists &amp; Agencies \u2013 Create custom voice models and collaborate virtually</li> <li>\u26bd Live Sports Broadcasters \u2013 Bring real-time multilingual commentary to global fans</li> <li>\ud83e\udde0 AI Developers &amp; Media Startups \u2013 Leverage our Dubbing-as-a-Service (DaaS) APIs</li> <li>\ud83d\udce2 Ad Agencies &amp; Brands \u2013 Run culturally adapted, emotion-driven ad campaigns</li> </ul>"},{"location":"#what-youll-find-in-this-documentation","title":"\ud83d\udd0d What You'll Find in This Documentation","text":"<ul> <li>\ud83d\udcd6 Project Overview</li> <li>\ud83d\udd27 System Architecture</li> <li>\ud83e\udde0 AI Models Used</li> <li>\u2699\ufe0f API Reference</li> <li>\ud83e\uddea Demo Use Cases</li> <li>\ud83d\udee0\ufe0f Deployment &amp; Integration Guides</li> </ul> <p>Whether you're here to explore the tech or integrate it into your pipeline\u2014you're in the right place.</p> <p>Let the voices sync, and the stories soar. \ud83c\udf0d\ud83c\udf99\ufe0f</p>"},{"location":"examples/","title":"Examples","text":"<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> <p>add_numbers.py<pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> <pre><code># Function to add two numbers\ndef add_two_numbers(num1, num2):\n    return num1 + num2\n\n# Example usage\nresult = add_two_numbers(5, 3)\nprint('The sum is:', result)\n</code></pre> code-examples.md<pre><code>// Function to concatenate two strings\nfunction concatenateStrings(str1, str2) {\n  return str1 + str2;\n}\n\n// Example usage\nconst result = concatenateStrings(\"Hello, \", \"World!\");\nconsole.log(\"The concatenated string is:\", result);\n</code></pre> <pre><code>// Function to concatenate two strings\nfunction concatenateStrings(str1, str2) {\n  return str1 + str2;\n}\n\n// Example usage\nconst result = concatenateStrings(\"Hello, \", \"World!\");\nconsole.log(\"The concatenated string is:\", result);\n</code></pre></p>"},{"location":"examples/#content-tabs","title":"Content Tabs","text":"<p>This is some examples of content tabs.</p>"},{"location":"examples/#generic-content","title":"Generic Content","text":"Plain textUnordered listOrdered list <p>This is some plain text</p> <ul> <li>First item</li> <li>Second item</li> <li>Third item</li> </ul> <ol> <li>First item</li> <li>Second item</li> <li>Third item</li> </ol>"},{"location":"examples/#code-blocks-in-content-tabs","title":"Code Blocks in Content Tabs","text":"PythonJavaScript <pre><code>def main():\n    print(\"Hello world!\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <pre><code>function main() {\n    console.log(\"Hello world!\");\n}\n\nmain();\n</code></pre> <p>Collapsible callout:</p> Collapsible callout <p>Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.</p>"},{"location":"examples/#diagram-examples","title":"Diagram Examples","text":""},{"location":"examples/#flowcharts","title":"Flowcharts","text":"<pre><code>graph LR\n  A[Start] --&gt; B{Failure?};\n  B --&gt;|Yes| C[Investigate...];\n  C --&gt; D[Debug];\n  D --&gt; B;\n  B ----&gt;|No| E[Success!];</code></pre>"},{"location":"phases/phase1_mvp/","title":"\ud83d\udd25 Phase 1: Minimum Viable Product (MVP)","text":"<p>The MVP phase focuses on validating core dubbing and localization capabilities by integrating state-of-the-art models with minimal orchestration. This phase is hands-on and experimentation-heavy, enabling us to evaluate models individually before building full automation in future phases.</p>"},{"location":"phases/phase1_mvp/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Explore and evaluate pretrained models for:</li> <li>Speech-to-text (Whisper, Google ASR)</li> <li>Translation (mT5, MarianMT)</li> <li>Emotion recognition (BiLSTM, AudioSet-based CNN)</li> <li>Voice synthesis (Tacotron2, VITS, ElevenLabs,Fastspeech)</li> <li>Lip syncing (Wav2Lip, SyncNet)</li> <li>Understand compatibility of intermediate outputs across stages</li> <li>Simulate the full dubbing flow by manually stitching outputs</li> <li>Identify model strengths, limitations, and domain tuning needs</li> </ul>"},{"location":"phases/phase1_mvp/#key-activities","title":"\ud83d\udee0\ufe0f Key Activities","text":"<ul> <li>Collect diverse multimedia inputs (video/audio/text samples)</li> <li>Manually run each model and store outputs</li> <li>Document sample inputs, outputs, formats, and transformations</li> <li>Track latency, accuracy, and subjective quality</li> <li>Log known challenges or friction between model handovers</li> <li>Create reusable scripts for preprocessing and evaluation</li> </ul>"},{"location":"phases/phase1_mvp/#mvp-evaluation-metrics","title":"\ud83e\uddea MVP Evaluation Metrics","text":"<ul> <li>Transcription accuracy (WER)</li> <li>Translation fidelity (BLEU, subjective review)</li> <li>Emotion match rate (human annotated)</li> <li>Voice clarity &amp; naturalness (MOS score or rating scale)</li> <li>Lip sync believability (frame match ratio or user score)</li> </ul>"},{"location":"phases/phase1_mvp/#deliverables","title":"\ud83d\udce6 Deliverables","text":"<ul> <li>Evaluation reports for each model</li> <li>Test dataset and results repository</li> <li>Manual flow reference implementation</li> <li>Recommendations for Phase 2 orchestration</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/","title":"Overview of Vocasync UI","text":"<p>To implement an AI-powered video dubbing and voice cloning system, built using Gradio for the user interface. The pipeline takes an input video, extracts its audio, segments it into smaller chunks based on transcription timestamps, translates each segment into the target language, and then generates dubbed speech for each segment using advanced text-to-speech (TTS) models. It supports two modes of synthesis: original speaker cloning to preserve the speaker\u2019s identity, and emotion cloning where a separate reference audio is used to transfer a specific emotional tone.The final result is a properly clubbed cloned audio tracks of both original audio clone and emotion reference audio clone.</p>"},{"location":"phases/phase1_mvp/AWS_setup/#vocasync-emotion-pipeline","title":"VocaSync Emotion Pipeline","text":""},{"location":"phases/phase1_mvp/AWS_setup/#1-video-input","title":"1. Video Input","text":"<p>Inputs (Gradio UI)</p> <ul> <li>Video Input (MP4 file) \u2192 Source video from which audio is extracted.</li> <li>Reference Emotion Audio \u2192 Used if specific emotional style is needed in dubbing.</li> <li>Source Language \u2192 Language of the original audio (e.g., hi, en, te, ta, ur).</li> <li>Target Language \u2192 Language into which speech will be translated and dubbed (e.g., en).</li> <li>Whisper Model Size \u2192 Choice of transcription model (tiny, base, small, medium) depending on accuracy vs. speed.</li> <li>TTS Model Type \u2192 Choice of cloning model (YourTTS, XTTS) for generating cloned voice output.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#2-audio-extraction","title":"2. Audio Extraction","text":"<p>Library: MoviePy</p> <ul> <li>Extracts audio from the uploaded MP4 video using MoviePy library and saves it as .wav for further processing.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#3-audio-transcription-segmentation","title":"3. Audio Transcription &amp; Segmentation","text":"<p>Model: Whisper</p> <ul> <li>The extracted audio is transcribed into text using Whisper.</li> <li>Whisper is a transcription model developed by OpenAI, designed to transcribe audio in multiple languages, including Indian regional languages.</li> <li>It supports various model sizes such as tiny, small, medium, and large, where larger models provide higher accuracy and better multilingual support.</li> <li>For Indian languages, it is recommended to use Whisper Medium or above, as these can effectively detect the spoken language, generate accurate transcriptions, and produce timestamps for each spoken segment.</li> <li>Since not all modules efficiently handle long audios or lengthy text sequences, the extracted text is further segmented based on Whisper\u2019s timestamps.</li> <li>This timestamp-based segmentation ensures that each portion of audio is transcribed, translated, and processed in manageable units.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#4-text-translation","title":"4. Text Translation","text":"<p>Model: GoogleTranslator (deep-translator)</p> <ul> <li>Translates each transcribed segment into the target language.</li> <li>Works segment-wise to preserve timing and flow.</li> <li>Prevents errors and context loss in long passages.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#5-voice-cloning","title":"5. Voice Cloning","text":"<p>Model: Coqui TTS (XTTS v2 / YourTTS)</p> <ul> <li>The translated text is cloned using audio references.</li> <li> <p>Two audio inputs can be used:</p> <ul> <li>Original video audio \u2192 to preserve the speaker\u2019s identity.</li> <li>Emotion reference audio \u2192 to add a specific emotion.</li> </ul> </li> <li> <p>XTTS v2 provides better performance than YourTTS, producing highly similar cloned audio while retaining emotional tone from the original.</p> </li> <li>YourTTS can be used when explicit emotion transfer is required.</li> <li>Since TTS models support only about 300 words per generation, the segment-based method is used to process large audios efficiently.</li> <li>Each segment is cloned separately, ensuring both speaker identity and emotion consistency are maintained in the dubbed output.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#6segment-stitching","title":"6.Segment Stitching","text":"<p>Library: MoviePy + NumPy</p> <ul> <li>All cloned segments (original clone and emotion clone) are stitched back together.</li> <li>Silence padding is added where needed so the audio aligns perfectly with the original timestamps (should optimize this).</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#7-audio-similarity-analysis","title":"7. Audio Similarity Analysis","text":"<p>Library: Resemblyzer</p> <ul> <li>The speaker embeddings of original audio and cloned audio are compared.</li> <li>A similarity score is calculated to measure how closely the cloned voice matches the original speaker.</li> <li>Similarity score is calculated for both original audio and emotion reference audio.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#8-final-output-generation","title":"8. Final Output Generation","text":"<p>Library: Gradio</p> <ul> <li>The user receives two outputs:</li> <li>Original Speaker Clone Audio (translated speech in the speaker\u2019s voice).</li> <li>Emotion-reference clone Audio (translated speech with emotion reference).</li> <li>Results (audio files, transcription (both original and translated text) , similarity scores) are displayed in the Gradio interface.</li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#aws-setup","title":"AWS Setup","text":""},{"location":"phases/phase1_mvp/AWS_setup/#1-launch-ec2-instance","title":"1. Launch EC2 Instance","text":""},{"location":"phases/phase1_mvp/AWS_setup/#11-choose-instance","title":"\ud83d\ude80 1.1 Choose Instance","text":"<p>Before deploying VocaSync on AWS, choose an EC2 instance that meets the following requirements:</p> Requirement Recommended Instance Type <code>g4dn.xlarge</code> (GPU-enabled for faster Whisper + XTTS)  or <code>t3.xlarge</code> (CPU-only, slower) Operating System Ubuntu 22.04 LTS vCPUs / RAM 4 vCPUs / 16GB RAM minimum Storage 30 GB EBS (gp3 recommended)"},{"location":"phases/phase1_mvp/AWS_setup/#12-security-group","title":"\ud83d\udd10 1.2 Security Group","text":"<p>Configure the Security Group for your EC2 instance to allow the following inbound rules:</p> Port Protocol Purpose 22 TCP SSH access to the instance 7860 TCP Access to the Gradio Web UI 80 TCP (Optional) Allow HTTP traffic (for Nginx or a reverse proxy) 443 TCP (Optional) Allow HTTPS traffic (for secure Nginx reverse proxy)"},{"location":"phases/phase1_mvp/AWS_setup/#2-instance-setup-auto-start","title":"\u2699\ufe0f 2. Instance Setup &amp; Auto-Start","text":"<p>This step covers setting up your EC2 instance so that your Gradio app (<code>app.py</code>) runs automatically on every boot.</p>"},{"location":"phases/phase1_mvp/AWS_setup/#21-prepare-the-instance","title":"\ud83d\udda5\ufe0f 2.1 Prepare the Instance","text":"<p>SSH into your EC2 Instance:</p> <pre><code>bash\nssh -i your-key.pem ec2-user@&lt;your-ec2-public-ip&gt;\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#22-ensure-your-environment-is-ready","title":"\ud83d\udda5\ufe0f 2.2 Ensure Your Environment is Ready","text":"<p>Before proceeding, make sure the following are already set up on your EC2 instance:</p> <ul> <li>\u2705 <code>app.py</code> and all required dependencies are installed in <code>/home/ec2-user/</code></li> <li>\u2705 A Python virtual environment exists at <code>/home/ec2-user/gradio_env/</code></li> </ul>"},{"location":"phases/phase1_mvp/AWS_setup/#23-create-start-script","title":"\ud83c\udfc3 2.3 Create Start Script","text":"<p>Create a file called <code>start_gradio.sh</code> in <code>/home/ec2-user/</code>:</p> <pre><code>bash\nnano /home/ec2-user/start_gradio.sh\n</code></pre> <pre><code>cd /home/ec2-user/\nsource /home/ec2-user/gradio_env/bin/activate\npython3 app.py\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#create-and-enable-systemd-service","title":"Create and Enable Systemd Service","text":"<p>To ensure that your Gradio app automatically starts when the instance boots, create a systemd service.</p>"},{"location":"phases/phase1_mvp/AWS_setup/#create-the-service-file","title":"Create the Service File","text":"<p>Run the following command to create a new service file:</p> <pre><code>bash\nsudo nano /etc/systemd/system/gradio.service\n</code></pre> <pre><code>[Unit]\nDescription=Gradio App\nAfter=network.target\n\n[Service]\nUser=ec2-user\nWorkingDirectory=/home/ec2-user\nExecStart=/home/ec2-user/start_gradio.sh\nRestart=always\n\n[Install]\nWantedBy=multi-user.target\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#3-modify-for-public-access","title":"\ud83d\udda5 3. Modify for Public Access","text":"<p>To make the Gradio UI accessible from your EC2 instance\u2019s public IP, edit <code>vocasync.py</code> and modify the <code>ui.launch()</code> section as follows:</p> <pre><code>python\nif __name__ == \"__main__\":\n    if os.path.exists(TEMP_DIR):\n        shutil.rmtree(TEMP_DIR)\n    os.makedirs(TEMP_DIR, exist_ok=True)\n\n    # Launch Gradio publicly\n    ui.launch(\n        server_name=\"0.0.0.0\",  # Listen on all network interfaces\n        server_port=7860,       # Port exposed in Security Group\n        share=False,            # Disable Gradio's share link (optional)\n        debug=True\n    )\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#4-optional-domain-https-production","title":"\ud83c\udf10 4. Optional \u2013 Domain + HTTPS (Production)","text":"<p>For production deployments, you can expose your Gradio app securely via Nginx and Let's Encrypt (Certbot).</p>"},{"location":"phases/phase1_mvp/AWS_setup/#41-install-nginx-certbot","title":"\ud83d\udce6 4.1 Install Nginx + Certbot","text":"<p>Run the following commands:  </p> <pre><code>bash\nsudo apt-get install -y nginx certbot python3-certbot-nginx\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#42-configure-reverse-proxy","title":"\u2699\ufe0f 4.2 Configure Reverse Proxy","text":"<p>Create a new Nginx configuration file:  </p> <pre><code>bash\nsudo nano /etc/nginx/sites-available/vocasync\n</code></pre> <pre><code>server {\n    listen 80;\n    server_name yourdomain.com;\n\n    location / {\n        proxy_pass http://127.0.0.1:7860;\n        proxy_set_header Host $host;\n        proxy_set_header X-Real-IP $remote_addr;\n        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n    }\n}\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#43-enable-site-and-restart-nginx","title":"\ud83d\udd17 4.3 Enable Site and Restart Nginx **","text":"<p>Run the following commands:  </p> <pre><code>sudo ln -s /etc/nginx/sites-available/vocasync /etc/nginx/sites-enabled/\nsudo nginx -t\nsudo systemctl restart nginx\n</code></pre>"},{"location":"phases/phase1_mvp/AWS_setup/#5-running-the-application","title":"\u25b6\ufe0f 5. Running the Application","text":"<p>Run your application manually with:  </p> <pre><code>bash\npython app.py\n</code></pre> <p>Running on public URL: https://1234abcd.gradio.live</p> <p>You can access the VocaSync UI in two ways:</p> <ul> <li> <p>Via the Gradio link shown in the terminal</p> </li> <li> <p>Directly from your EC2 Public IPv4</p> </li> </ul> <pre><code>ui.launch(server_name=\"0.0.0.0\", server_port=7860)\n</code></pre> <p>then simply open:</p> <p><pre><code>http://&lt;your-ec2-public-ip&gt;:7860\n</code></pre> Or click on the Public IPv4 address shown in your AWS EC2 console, append :7860 at the end, and the VocaSync UI will open in your browser</p>"},{"location":"phases/phase1_mvp/gcp_setup/","title":"Overview of Vocasync UI","text":"<p>To implement an AI-powered video dubbing and voice cloning system, built using Gradio for the user interface. The pipeline takes an input video, extracts its audio, segments it into smaller chunks based on transcription timestamps, translates each segment into the target language, and then generates dubbed speech for each segment using advanced text-to-speech (TTS) models. It supports two modes of synthesis: original speaker cloning to preserve the speaker\u2019s identity, and emotion cloning where a separate reference audio is used to transfer a specific emotional tone.The final result is a properly clubbed cloned audio tracks of both original audio clone and emotion reference audio clone.</p>"},{"location":"phases/phase1_mvp/gcp_setup/#vocasync-emotion-pipeline","title":"VocaSync Emotion Pipeline","text":""},{"location":"phases/phase1_mvp/gcp_setup/#1-video-input","title":"1. Video Input","text":"<p>Inputs (Gradio UI)</p> <ul> <li>Video Input (MP4 file) \u2192 Source video from which audio is extracted.</li> <li>Reference Emotion Audio \u2192 Used if specific emotional style is needed in dubbing.</li> <li>Source Language \u2192 Language of the original audio (e.g., hi, en, te, ta, ur).</li> <li>Target Language \u2192 Language into which speech will be translated and dubbed (e.g., en).</li> <li>Whisper Model Size \u2192 Choice of transcription model (tiny, base, small, medium) depending on accuracy vs. speed.</li> <li>TTS Model Type \u2192 Choice of cloning model (YourTTS, XTTS) for generating cloned voice output.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#2-audio-extraction","title":"2. Audio Extraction","text":"<p>Library: MoviePy</p> <ul> <li>Extracts audio from the uploaded MP4 video using MoviePy library and saves it as .wav for further processing.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#3-audio-transcription-segmentation","title":"3. Audio Transcription &amp; Segmentation","text":"<p>Model: Whisper</p> <ul> <li>The extracted audio is transcribed into text using Whisper.</li> <li>Whisper is a transcription model developed by OpenAI, designed to transcribe audio in multiple languages, including Indian regional languages.</li> <li>It supports various model sizes such as tiny, small, medium, and large, where larger models provide higher accuracy and better multilingual support.</li> <li>For Indian languages, it is recommended to use Whisper Medium or above, as these can effectively detect the spoken language, generate accurate transcriptions, and produce timestamps for each spoken segment.</li> <li>Since not all modules efficiently handle long audios or lengthy text sequences, the extracted text is further segmented based on Whisper\u2019s timestamps.</li> <li>This timestamp-based segmentation ensures that each portion of audio is transcribed, translated, and processed in manageable units.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#4-text-translation","title":"4. Text Translation","text":"<p>Model: GoogleTranslator (deep-translator)</p> <ul> <li>Translates each transcribed segment into the target language.</li> <li>Works segment-wise to preserve timing and flow.</li> <li>Prevents errors and context loss in long passages.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#5-voice-cloning","title":"5. Voice Cloning","text":"<p>Model: Coqui TTS (XTTS v2 / YourTTS)</p> <ul> <li>The translated text is cloned using audio references.</li> <li> <p>Two audio inputs can be used:</p> <ul> <li>Original video audio \u2192 to preserve the speaker\u2019s identity.</li> <li>Emotion reference audio \u2192 to add a specific emotion.</li> </ul> </li> <li> <p>XTTS v2 provides better performance than YourTTS, producing highly similar cloned audio while retaining emotional tone from the original.</p> </li> <li>YourTTS can be used when explicit emotion transfer is required.</li> <li>Since TTS models support only about 300 words per generation, the segment-based method is used to process large audios efficiently.</li> <li>Each segment is cloned separately, ensuring both speaker identity and emotion consistency are maintained in the dubbed output.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#6segment-stitching","title":"6.Segment Stitching","text":"<p>Library: MoviePy + NumPy</p> <ul> <li>All cloned segments (original clone and emotion clone) are stitched back together.</li> <li>Silence padding is added where needed so the audio aligns perfectly with the original timestamps (should optimize this).</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#7-audio-similarity-analysis","title":"7. Audio Similarity Analysis","text":"<p>Library: Resemblyzer</p> <ul> <li>The speaker embeddings of original audio and cloned audio are compared.</li> <li>A similarity score is calculated to measure how closely the cloned voice matches the original speaker.</li> <li>Similarity score is calculated for both original audio and emotion reference audio.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#8-final-output-generation","title":"8. Final Output Generation","text":"<p>Library: Gradio</p> <ul> <li>The user receives two outputs:</li> <li>Original Speaker Clone Audio (translated speech in the speaker\u2019s voice).</li> <li>Emotion-reference clone Audio (translated speech with emotion reference).</li> <li>Results (audio files, transcription (both original and translated text) , similarity scores) are displayed in the Gradio interface.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#setup","title":"Setup","text":""},{"location":"phases/phase1_mvp/gcp_setup/#step-1-upload-the-notebook","title":"Step 1: Upload the Notebook","text":"<ul> <li>Go to Colab \u2192 File \u2192 Upload notebook.</li> <li>Upload the UI/GCP_UI/UI.ipynb notebook</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-2-change-runtime-type","title":"Step 2: Change runtime type","text":"<ul> <li>Make sure to use a GPU runtime as whisper medium/large and TTS works better with GPU:<ul> <li>Runtime \u2192 Change runtime type</li> <li>You can select a T4 GPU or any available GPU.</li> </ul> </li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-3-check-python-version","title":"Step 3: Check Python Version","text":"<ul> <li>Run this cell at the top of the notebook:     <pre><code>import sys\nprint(\"Current Python version:\", sys.version)\n</code></pre></li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-4-run-the-code","title":"Step 4: Run the code","text":"<ul> <li>If Python \u2264 3.11: \u2705 You can continue running the notebook normally.</li> <li>If Python &gt; 3.11 (e.g., 3.12): \u26a0\ufe0f TTS will fail, so you need to downgrade to Python 3.10/3.11.</li> <li>Code for both cases is written in the file so just run those cells according to the instructions mentioned in the notebook.</li> <li>When prompted with: Do you want to install TTS? [y/n] \u2192 type y.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-5-launch-gradio","title":"Step 5: Launch Gradio","text":"<ul> <li>After running all cells, the Gradio interface will launch.</li> <li>Colab will display two links:<ul> <li>Public Gradio link (https)</li> <li>Localhost link (http)</li> </ul> </li> <li>Open either link in your browser.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-6-use-the-interface","title":"Step 6: Use the Interface","text":"<ul> <li>Upload an MP4 video.</li> <li>Optional: upload reference emotion audio (if you have one).</li> <li>Select source language, target language, Whisper model, and TTS model.</li> <li>Click \u201cGenerate Dub\u201d.</li> </ul>"},{"location":"phases/phase1_mvp/gcp_setup/#step-7-view-outputs","title":"Step 7: View Outputs","text":"<ul> <li>Gradio will show:<ul> <li>Transcribed text</li> <li>Translated text</li> <li>Cloned audio (original speaker)</li> <li>Emotion/reference speaker audio (if reference audio was provided)</li> <li>Similarity scores</li> </ul> </li> </ul>"},{"location":"phases/phase1_mvp/kimai/","title":"Kimai Server","text":"<ul> <li>Kimai is an open-source, web-based time tracking tool that helps track work hours, manage projects/clients, and generate reports or invoices.</li> <li>Its purpose is to make time management and billing easier for freelancers, teams, and companies through a simple browser-based interface.</li> <li>We cloned the Kimai GitHub project into our EC2 instance and used MariaDB as the backend database to store user and project data.</li> <li>Nginx was configured as the web server, working with PHP-FPM to process Kimai\u2019s PHP code and serve the application.</li> <li>We enabled access through port 443 (HTTPS) so that Kimai could be opened securely using the EC2 instance\u2019s public IP address.</li> <li>We then added users, projects, and activities, creating member accounts so they could log their working hours, while the admin could monitor all activity and reports.</li> </ul>"},{"location":"phases/phase1_mvp/kimai/#instance-name-test2","title":"Instance Name: test2","text":""},{"location":"phases/phase1_mvp/kimai/#admin-credentials","title":"Admin Credentials:","text":"<ul> <li>Username: admin</li> <li>Password: kimaiadmin</li> </ul>"},{"location":"phases/phase1_mvp/kimai/#mariadb-credentials","title":"MariaDB credentials:","text":"<ul> <li>user: \"kimaiuser\"</li> <li>password: \"StrongPassword123!\"</li> <li>database: \"kimai2\u201d</li> </ul>"},{"location":"phases/phase1_mvp/kimai/#to-restore-kimai-db-use-following-command","title":"To Restore kimai db use following command:","text":"<pre><code>mysql -u kimaiuser -p kimai2 &lt; kimai_db_backup.sql\n</code></pre>"},{"location":"phases/phase1_mvp/kimai/#to-restore-files-use-following-command","title":"To restore files use following command:","text":"<pre><code>tar -xzf kimai_files_backup.tar.gz -C /var/www/ \n</code></pre>"},{"location":"phases/phase1_mvp/kimai/#sending-user-report-to-admin","title":"Sending User report to Admin","text":"<ul> <li>Connected to Kimai Database \u2013 Used mysql.connector to connect directly to the kimai2 database and fetch weekly and total worked hours of users.</li> <li>Processed Timesheet Data \u2013 Wrote SQL queries to calculate each user\u2019s weekly work duration and overall total work duration.</li> <li>Generated PDF Report \u2013 Used the fpdf library to format the data into a tabular report, including user, project, activity, weekly time, and total time.</li> <li>Saved the Report \u2013 Exported the generated report as a PDF file (kimai_db_weekly_report.pdf).</li> <li>Configured Email Setup \u2013 Used Python\u2019s smtplib with Gmail SMTP server (smtp.gmail.com) and app password authentication.</li> <li>Sent Report to Admin \u2013 Attached the PDF report to an email and automatically sent it to the admin (sanjanaproject36@gmail.com).</li> <li>Configured a cron job to automatically execute the report generation and email script every time we run below code, with output logged for monitoring.</li> </ul> <pre><code>* ** * * * /usr/bin/python3 /home/ec2-user/kimai_db_report.py &gt;&gt; /home/ec2-user/kimai_cron.log 2&gt;&amp;1 \n</code></pre> <ul> <li>Manual Execution Option \u2013 Provided the flexibility to run the script manually whenever required (e.g., by executing python3 kimai_db_report.py).</li> </ul>"},{"location":"phases/phase1_mvp/kimai/#changes-need-to-be-done-in-kimai_db_reportpy","title":"Changes need to be done in kimai_db_report.py:","text":"<pre><code>gmail_user = \"yelagandulasupraja@gmail.com\"    # replace it with sender mail\ngmail_app_password = \"rvfrdedhonyhjdea\"          #create a password with sendermail using this link -- https://myaccount.google.com/apppasswords (sender mail must have two step verification to create password)\nadmin_email = \"sanjanaproject36@gmail.com\"     #replace it with receiver mail\n</code></pre>"},{"location":"phases/phase1_mvp/lip_sync/","title":"\ud83d\udc44 Lip-Syncing Pipeline \u2013 MVP Phase","text":"<p>This section documents the lip-syncing pipeline activities under the MVP phase. We use Wav2Lip as our base model and enhance it for multilingual, emotion-aware, and accurate synchronization between AI-generated speech and video frames.</p>"},{"location":"phases/phase1_mvp/lip_sync/#objectives","title":"\ud83c\udfaf Objectives","text":"<ul> <li>Train and test the Wav2Lip model.</li> <li>Build evaluation metrics for lip-sync accuracy.</li> <li>Adapt lip-syncing for different languages and accents.</li> <li>Optimize outputs to look natural and emotionally accurate.</li> </ul>"},{"location":"phases/phase1_mvp/lip_sync/#1-train-test-wav2lip","title":"\ud83d\udee0\ufe0f 1. Train &amp; Test Wav2Lip","text":""},{"location":"phases/phase1_mvp/lip_sync/#overview","title":"Overview","text":"<p>Use Wav2Lip to match AI-generated speech with lip movements in video frames.</p>"},{"location":"phases/phase1_mvp/lip_sync/#activities","title":"Activities","text":"<ol> <li>Collect diverse video samples (with different speakers, languages, accents, and emotions).</li> <li>Extract audio and video using tools like <code>ffmpeg</code>.</li> <li>Preprocess and align the audio with video frames.</li> <li>Train Wav2Lip using default weights or fine-tune on your data.</li> <li>Test and generate outputs for validation.</li> </ol> <pre><code>graph LR\n    A[Collect Videos] --&gt; B[Extract Audio &amp; Video]\n    B --&gt; C[Align Audio and Video]\n    C --&gt; D[Train Wav2Lip]\n    D --&gt; E[Test Outputs]\n    E --&gt; F[Generate Lip-Synced Video]</code></pre>"},{"location":"phases/phase1_mvp/lip_sync/#2-develop-evaluation-metrics","title":"\ud83d\udccf 2. Develop Evaluation Metrics","text":""},{"location":"phases/phase1_mvp/lip_sync/#objective","title":"\ud83c\udfaf Objective","text":"<p>Create measurable ways to assess how well lips sync with speech.</p>"},{"location":"phases/phase1_mvp/lip_sync/#activities_1","title":"\ud83d\udee0\ufe0f Activities","text":""},{"location":"phases/phase1_mvp/lip_sync/#objective-tests","title":"\u2705 Objective Tests","text":"<ul> <li>Use DeepFaceLab or OpenCV to track mouth movement.</li> <li>Calculate frame alignment and mouth openness vs. phoneme timeline.</li> </ul>"},{"location":"phases/phase1_mvp/lip_sync/#subjective-tests","title":"\u2705 Subjective Tests","text":"<ul> <li>Use Mean Opinion Scores (MOS) from human reviewers on a 1\u20135 scale for:</li> <li>Realism  </li> <li>Sync quality  </li> <li>Naturalness</li> </ul> <pre><code>graph LR\n    A[Track Lips with OpenCV] --&gt; B[Calculate Frame Alignment]\n    B --&gt; C[Human Evaluation ]\n    C --&gt; D[Overall Lip Sync Score]</code></pre>"},{"location":"phases/phase1_mvp/lip_sync/#3-handle-languages-accents","title":"\ud83c\udf10 3. Handle Languages &amp; Accents","text":""},{"location":"phases/phase1_mvp/lip_sync/#objective_1","title":"\ud83c\udfaf Objective","text":"<p>Ensure that lip-syncing works accurately across different languages, dialects, and phonetic structures.</p>"},{"location":"phases/phase1_mvp/lip_sync/#activities_2","title":"\ud83d\udee0\ufe0f Activities","text":"<ul> <li>Collect a multilingual dataset (e.g., English, Spanish, Hindi).</li> <li>Understand language-specific phoneme structures and their variations.</li> <li>Fine-tune the Wav2Lip model for each language or dialect.</li> <li>Adjust synchronization timing to align with different speech cadences and syllable timings.</li> <li>Validate output videos through:</li> <li>Visual review by human evaluators.</li> <li>Feedback sessions with multilingual speakers.</li> </ul> <pre><code>graph LR\n    A[Collect Multilingual Data] --&gt; B[Phoneme Analysis]\n    B --&gt; C[Language-Specific Fine-Tuning]\n    C --&gt; D[Test Across Languages]</code></pre>"},{"location":"phases/phase1_mvp/lip_sync/#4-testing-optimization","title":"\ud83d\ude80 4. Testing &amp; Optimization","text":""},{"location":"phases/phase1_mvp/lip_sync/#objective_2","title":"\ud83c\udfaf Objective","text":"<p>Improve the overall performance and visual realism of the lip-synced outputs by refining model predictions, testing against real data, and optimizing post-processing workflows.</p>"},{"location":"phases/phase1_mvp/lip_sync/#4-testing-optimization_1","title":"\ud83d\ude80 4. Testing &amp; Optimization","text":""},{"location":"phases/phase1_mvp/lip_sync/#objective_3","title":"\ud83c\udfaf Objective","text":"<p>Improve the overall performance and visual realism of the lip-synced outputs by refining model predictions, testing against real data, and optimizing post-processing workflows.</p>"},{"location":"phases/phase1_mvp/lip_sync/#activities_3","title":"\ud83d\udee0\ufe0f Activities","text":"<ul> <li>Generate lip-synced videos using the trained Wav2Lip model on test samples.</li> <li>Compare the outputs with real human speech videos to assess:</li> <li>Mouth movement accuracy</li> <li>Facial emotion consistency</li> <li>Visual quality</li> <li>Post-process videos using tools like <code>ffmpeg</code> for:</li> <li>Frame stabilization</li> <li>Frame rate matching</li> <li>Audio sync smoothing</li> <li>Tune hyperparameters such as:</li> <li>Learning rate</li> <li>Epochs</li> <li>Batch size</li> <li>Audio-video alignment thresholds</li> <li>Retrain and test iteratively with improved, augmented, or diversified datasets.</li> </ul> <pre><code>graph LR\n    A[Generate Lip-Synced Videos] --&gt; B[Evaluate &amp; Score]\n    B --&gt; C[Optimize with Post-Processing]\n    C --&gt; D[Refine Model Hyperparameters]\n    D --&gt; E[Generate Final Output]</code></pre>"},{"location":"vocasync/project_overview/","title":"Project Overview","text":"<p>Welcome to the VocaSync Project Documentation! This project is designed to deliver hyper-personalized AI dubbing experiences using cutting-edge speech-to-speech, emotion recognition, and language localization models.</p>"},{"location":"vocasync/project_overview/#video-input-flow","title":"\ud83c\udfa5 Video Input Flow","text":"<p>Video Input Processing Flow</p> <p>This flow represents the full processing pipeline when the input is a video file. The system extracts both audio and  visual signals, processes them independently, and combines them using a lip-sync model to produce a fully dubbed and emotionally aligned video.</p> <pre><code>graph TD\n    A[Video Input File] --&gt; B[Extract Audio Track]\n    B --&gt; C[Speech to Speech Model]\n    A --&gt; D[Send Video Frames to Emotion Recognition]\n    C --&gt; E[Voice Synth Output]\n    D --&gt; F[Detected Facial Emotion]\n    E --&gt; G[Lip Sync Model]\n    F --&gt; G\n    G --&gt; H[Render Video with Dubbed Audio]\n    H --&gt; I[Final Output Delivery  File or API]</code></pre>"},{"location":"vocasync/project_overview/#livestream-audio-processing-flow","title":"\ud83d\udd0a Livestream Audio Processing Flow","text":"<p>The video input is processed in parallel: the audio is routed through a speech-to-speech model to generate a dubbed voice, while the visual frames are analyzed for emotional context. These two outputs are merged in the lip-sync model, producing a synchronized and emotionally accurate final video.</p> <p>Livestream Audio Processing Flow</p> <p>This pipeline outlines the real-time dubbing process for live audio streams, such as live sports, podcasts, or events. The input is passed through several stages to generate an emotionally resonant and language-localized dubbed stream.</p> <pre><code>graph TD\n    A[Live Audio Stream] --&gt; B[Speech to Text - Whisper or ASR]\n    B --&gt; C[Translation Engine]\n    C --&gt; D[Emotion Detection]\n    D --&gt; E[Voice Cloning and Synthesis]\n    E --&gt; F[Streamed Output Delivery via API or UI]</code></pre> <p>The live audio stream is first transcribed, translated into the target language, and passed through emotion detection before being synthesized into a human-like voice and streamed in real time. This ensures both linguistic accuracy and emotional fidelity.</p>"}]}